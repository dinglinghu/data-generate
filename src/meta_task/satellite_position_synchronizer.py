#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å«æ˜Ÿä½ç½®åŒæ­¥å™¨
ä¸ºæ¯ä¸ªå¯è§å…ƒä»»åŠ¡åŒæ­¥å«æ˜Ÿåœ¨ä»»åŠ¡æ—¶é—´æ®µå†…çš„ä½ç½®ä¿¡æ¯
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import json
import asyncio
import concurrent.futures
import threading
from functools import partial

from .parallel_position_manager import ParallelPositionManager, PositionRequest, PositionResult

logger = logging.getLogger(__name__)

class SatellitePositionSynchronizer:
    """å«æ˜Ÿä½ç½®åŒæ­¥å™¨"""
    
    def __init__(self, stk_manager, time_manager, config_manager=None):
        """
        åˆå§‹åŒ–å«æ˜Ÿä½ç½®åŒæ­¥å™¨

        Args:
            stk_manager: STKç®¡ç†å™¨å®ä¾‹
            time_manager: æ—¶é—´ç®¡ç†å™¨å®ä¾‹
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.stk_manager = stk_manager
        self.time_manager = time_manager

        # è·å–é…ç½®
        if config_manager:
            from ..utils.config_manager import get_config_manager
            self.config_manager = config_manager
        else:
            from ..utils.config_manager import get_config_manager
            self.config_manager = get_config_manager()

        # è·å–ä½ç½®åŒæ­¥é…ç½®
        meta_task_config = self.config_manager.config.get("meta_task_management", {})
        position_sync_config = meta_task_config.get("position_sync", {})

        # ä½ç½®é‡‡æ ·é…ç½®
        self.position_sample_interval = position_sync_config.get("sample_interval_seconds", 30)
        self.max_samples_per_task = position_sync_config.get("max_samples_per_task", 20)
        self.enable_statistics = position_sync_config.get("enable_statistics", True)
        self.export_detailed_data = position_sync_config.get("export_detailed_data", False)

        # å¹¶å‘é…ç½®
        self.enable_concurrent = position_sync_config.get("enable_concurrent", True)
        self.max_workers = position_sync_config.get("max_workers", 4)
        self.concurrent_batch_size = position_sync_config.get("concurrent_batch_size", 10)
        self.stk_com_timeout = position_sync_config.get("stk_com_timeout", 30)

        # STK COMæ¥å£çº¿ç¨‹å®‰å…¨é”
        self._stk_lock = threading.Lock()

        # åˆå§‹åŒ–å¹¶è¡Œä½ç½®ç®¡ç†å™¨
        self.parallel_position_manager = ParallelPositionManager(
            stk_manager=self.stk_manager,
            config_manager=self.config_manager
        )

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.enable_parallel_optimization = position_sync_config.get("enable_parallel_optimization", True)

        logger.info("ğŸ›°ï¸ å«æ˜Ÿä½ç½®åŒæ­¥å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é‡‡æ ·é—´éš”: {self.position_sample_interval}ç§’")
        logger.info(f"   æœ€å¤§é‡‡æ ·ç‚¹æ•°: {self.max_samples_per_task}")
        logger.info(f"   ç»Ÿè®¡è®¡ç®—: {'å¯ç”¨' if self.enable_statistics else 'ç¦ç”¨'}")
        logger.info(f"   å¹¶å‘å¤„ç†: {'å¯ç”¨' if self.enable_concurrent else 'ç¦ç”¨'}")
        logger.info(f"   å¹¶è¡Œä¼˜åŒ–: {'å¯ç”¨' if self.enable_parallel_optimization else 'ç¦ç”¨'}")
        if self.enable_concurrent:
            logger.info(f"   æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
            logger.info(f"   æ‰¹å¤„ç†å¤§å°: {self.concurrent_batch_size}")
    
    def synchronize_satellite_positions_for_visible_tasks(self, visible_meta_tasks: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸ºæ‰€æœ‰å¯è§å…ƒä»»åŠ¡åŒæ­¥å«æ˜Ÿä½ç½®ï¼ˆæ”¯æŒå¹¶å‘å¤„ç†ï¼‰

        Args:
            visible_meta_tasks: å¯è§å…ƒä»»åŠ¡æ•°æ®

        Returns:
            å¢å¼ºåçš„å¯è§å…ƒä»»åŠ¡æ•°æ®ï¼ˆåŒ…å«å«æ˜Ÿä½ç½®ä¿¡æ¯ï¼‰
        """
        try:
            start_time = datetime.now()
            logger.info("ğŸ›°ï¸ å¼€å§‹ä¸ºå¯è§å…ƒä»»åŠ¡åŒæ­¥å«æ˜Ÿä½ç½®...")

            enhanced_visible_tasks = visible_meta_tasks.copy()

            # è·å–æ˜Ÿåº§å¯è§ä»»åŠ¡é›†
            constellation_visible_task_sets = enhanced_visible_tasks.get("constellation_visible_task_sets", {})

            # é€‰æ‹©æœ€ä¼˜çš„å¤„ç†æ¨¡å¼
            if self.enable_parallel_optimization:
                logger.info("ğŸš€ ä½¿ç”¨å¹¶è¡Œä¼˜åŒ–æ¨¡å¼")
                total_tasks_processed, total_positions_collected = self._process_tasks_parallel_optimized(
                    constellation_visible_task_sets
                )
            elif self.enable_concurrent:
                logger.info("ğŸ§µ ä½¿ç”¨å¹¶å‘å¤„ç†æ¨¡å¼")
                total_tasks_processed, total_positions_collected = self._process_tasks_concurrently(
                    constellation_visible_task_sets
                )
            else:
                logger.info("ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç†æ¨¡å¼")
                total_tasks_processed, total_positions_collected = self._process_tasks_serially(
                    constellation_visible_task_sets
                )

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()

            # æ·»åŠ åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
            enhanced_visible_tasks["position_sync_metadata"] = {
                "sync_time": datetime.now().isoformat(),
                "total_tasks_processed": total_tasks_processed,
                "total_positions_collected": total_positions_collected,
                "sample_interval_seconds": self.position_sample_interval,
                "processing_time_seconds": processing_time,
                "concurrent_enabled": self.enable_concurrent,
                "max_workers": self.max_workers if self.enable_concurrent else 1,
                "sync_status": "completed"
            }

            logger.info(f"ğŸ›°ï¸ å«æ˜Ÿä½ç½®åŒæ­¥å®Œæˆ:")
            logger.info(f"   å¤„ç†ä»»åŠ¡æ•°: {total_tasks_processed}")
            logger.info(f"   é‡‡é›†ä½ç½®ç‚¹æ•°: {total_positions_collected}")
            logger.info(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            logger.info(f"   å¤„ç†æ¨¡å¼: {'å¹¶å‘' if self.enable_concurrent else 'ä¸²è¡Œ'}")

            return enhanced_visible_tasks

        except Exception as e:
            logger.error(f"âŒ å«æ˜Ÿä½ç½®åŒæ­¥å¤±è´¥: {e}")
            import traceback
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return visible_meta_tasks

    def _process_tasks_parallel_optimized(self, constellation_visible_task_sets: Dict[str, Any]) -> Tuple[int, int]:
        """
        ä½¿ç”¨å¹¶è¡Œä½ç½®ç®¡ç†å™¨ä¼˜åŒ–å¤„ç†æ‰€æœ‰å¯è§ä»»åŠ¡çš„ä½ç½®åŒæ­¥

        Args:
            constellation_visible_task_sets: æ˜Ÿåº§å¯è§ä»»åŠ¡é›†

        Returns:
            (å¤„ç†ä»»åŠ¡æ•°, é‡‡é›†ä½ç½®ç‚¹æ•°)
        """
        try:
            logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œä¼˜åŒ–ä½ç½®åŒæ­¥...")

            # 1. æ”¶é›†æ‰€æœ‰ä½ç½®è¯·æ±‚
            position_requests = []
            task_mapping = {}  # ç”¨äºæ˜ å°„è¯·æ±‚åˆ°ä»»åŠ¡

            for satellite_id, satellite_tasks in constellation_visible_task_sets.items():
                missile_tasks = satellite_tasks.get("missile_tasks", {})

                for missile_id, missile_task_data in missile_tasks.items():
                    visible_tasks = missile_task_data.get("visible_tasks", [])

                    for task in visible_tasks:
                        # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆä½ç½®è¯·æ±‚
                        task_requests = self._generate_position_requests_for_task(satellite_id, task)
                        position_requests.extend(task_requests)

                        # å»ºç«‹æ˜ å°„å…³ç³» - ä½¿ç”¨å”¯ä¸€é”®é¿å…task_idå†²çª
                        task_id = task.get("task_id")
                        unique_task_key = f"{satellite_id}_{missile_id}_{task_id}"
                        task_mapping[unique_task_key] = {
                            "satellite_id": satellite_id,
                            "missile_id": missile_id,
                            "task_id": task_id,
                            "task": task,
                            "request_indices": list(range(len(position_requests) - len(task_requests), len(position_requests)))
                        }

            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            expected_requests_old = len(task_mapping) * 11  # æ—§ç­–ç•¥ï¼šå¹³å‡æ¯ä»»åŠ¡11ä¸ªé‡‡æ ·ç‚¹ï¼ˆ30ç§’é—´éš”ï¼‰
            actual_requests = len(position_requests)
            optimization_ratio = (expected_requests_old - actual_requests) / max(1, expected_requests_old) * 100

            logger.info(f"ğŸ“Š æ”¶é›†åˆ° {actual_requests} ä¸ªä½ç½®è¯·æ±‚ï¼Œè¦†ç›– {len(task_mapping)} ä¸ªä»»åŠ¡")
            logger.info(f"ğŸš€ é‡‡æ ·ä¼˜åŒ–: é¢„æœŸ{expected_requests_old}ä¸ªè¯·æ±‚ â†’ å®é™…{actual_requests}ä¸ªè¯·æ±‚ (å‡å°‘{optimization_ratio:.1f}%)")
            logger.info(f"âš¡ ç­–ç•¥: æ¯ä¸ªå¯è§å…ƒä»»åŠ¡åªé‡‡é›†å¼€å§‹å’Œç»“æŸæ—¶åˆ»ä½ç½®ï¼Œå¤§å¹…åŠ é€Ÿæ•°æ®é‡‡é›†")

            # 2. å¹¶è¡Œè·å–æ‰€æœ‰ä½ç½®
            if position_requests:
                position_results = self.parallel_position_manager.get_positions_parallel(position_requests)

                # 3. ç»„ç»‡ç»“æœå¹¶æ›´æ–°ä»»åŠ¡
                total_tasks_processed = 0
                total_positions_collected = 0

                for unique_task_key, mapping_info in task_mapping.items():
                    satellite_id = mapping_info["satellite_id"]
                    missile_id = mapping_info["missile_id"]
                    task_id = mapping_info["task_id"]
                    task = mapping_info["task"]
                    request_indices = mapping_info["request_indices"]

                    # æå–è¯¥ä»»åŠ¡çš„ä½ç½®ç»“æœ
                    task_position_results = [position_results[i] for i in request_indices if i < len(position_results)]

                    # æ„å»ºä½ç½®åŒæ­¥æ•°æ®
                    position_sync_data = self._build_position_sync_data_from_results(
                        satellite_id, task, task_position_results
                    )

                    if position_sync_data:
                        task["satellite_position_sync"] = position_sync_data
                        total_positions_collected += len(position_sync_data.get("position_samples", []))
                        total_tasks_processed += 1
                        logger.debug(f"âœ… ä»»åŠ¡ {satellite_id}-{missile_id}-{task_id} å¹¶è¡Œä½ç½®åŒæ­¥å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ ä»»åŠ¡ {satellite_id}-{missile_id}-{task_id} å¹¶è¡Œä½ç½®åŒæ­¥å¤±è´¥")

                # 4. è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
                parallel_stats = self.parallel_position_manager.get_stats()
                logger.info("ğŸ“Š å¹¶è¡Œä½ç½®åŒæ­¥æ€§èƒ½ç»Ÿè®¡:")
                logger.info(f"   æ€»è¯·æ±‚æ•°: {parallel_stats.get('total_requests', 0)}")
                logger.info(f"   æˆåŠŸç‡: {parallel_stats.get('success_rate', 0):.1f}%")
                logger.info(f"   ç¼“å­˜å‘½ä¸­ç‡: {parallel_stats.get('cache_hit_rate', 0):.1f}%")
                logger.info(f"   å¹³å‡å¤„ç†æ—¶é—´: {parallel_stats.get('average_time_per_request', 0):.3f}s")

                return total_tasks_processed, total_positions_collected
            else:
                logger.warning("âš ï¸ æ²¡æœ‰ä½ç½®è¯·æ±‚éœ€è¦å¤„ç†")
                return 0, 0

        except Exception as e:
            logger.error(f"âŒ å¹¶è¡Œä¼˜åŒ–ä½ç½®åŒæ­¥å¤±è´¥: {e}")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            logger.info("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†æ¨¡å¼...")
            return self._process_tasks_serially(constellation_visible_task_sets)

    def _process_tasks_concurrently(self, constellation_visible_task_sets: Dict[str, Any]) -> Tuple[int, int]:
        """
        å¹¶å‘å¤„ç†æ‰€æœ‰å¯è§ä»»åŠ¡çš„ä½ç½®åŒæ­¥

        Args:
            constellation_visible_task_sets: æ˜Ÿåº§å¯è§ä»»åŠ¡é›†

        Returns:
            (å¤„ç†ä»»åŠ¡æ•°, é‡‡é›†ä½ç½®ç‚¹æ•°)
        """
        try:
            # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„ä»»åŠ¡
            task_items = []

            for satellite_id, satellite_tasks in constellation_visible_task_sets.items():
                missile_tasks = satellite_tasks.get("missile_tasks", {})

                for missile_id, missile_task_data in missile_tasks.items():
                    visible_tasks = missile_task_data.get("visible_tasks", [])

                    for task in visible_tasks:
                        task_items.append((satellite_id, task))

            logger.info(f"ğŸš€ å¹¶å‘å¤„ç† {len(task_items)} ä¸ªå¯è§ä»»åŠ¡ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")

            total_tasks_processed = 0
            total_positions_collected = 0

            # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡å¤šçš„å¹¶å‘è¿æ¥
            batch_size = self.concurrent_batch_size

            for i in range(0, len(task_items), batch_size):
                batch = task_items[i:i + batch_size]
                logger.debug(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªä»»åŠ¡")

                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_task = {
                        executor.submit(self._synchronize_position_for_task_thread_safe, satellite_id, task): (satellite_id, task)
                        for satellite_id, task in batch
                    }

                    # æ”¶é›†ç»“æœ
                    for future in concurrent.futures.as_completed(future_to_task):
                        satellite_id, task = future_to_task[future]
                        try:
                            position_data = future.result()

                            if position_data:
                                task["satellite_position_sync"] = position_data
                                total_positions_collected += len(position_data.get("position_samples", []))
                                total_tasks_processed += 1

                                logger.debug(f"âœ… ä»»åŠ¡ {task.get('task_id')} ä½ç½®åŒæ­¥å®Œæˆ")
                            else:
                                logger.warning(f"âš ï¸ ä»»åŠ¡ {task.get('task_id')} ä½ç½®åŒæ­¥å¤±è´¥")

                        except Exception as e:
                            logger.error(f"âŒ ä»»åŠ¡ {task.get('task_id')} å¤„ç†å¼‚å¸¸: {e}")

            return total_tasks_processed, total_positions_collected

        except Exception as e:
            logger.error(f"âŒ å¹¶å‘å¤„ç†å¤±è´¥: {e}")
            return 0, 0

    def _process_tasks_serially(self, constellation_visible_task_sets: Dict[str, Any]) -> Tuple[int, int]:
        """
        ä¸²è¡Œå¤„ç†æ‰€æœ‰å¯è§ä»»åŠ¡çš„ä½ç½®åŒæ­¥ï¼ˆåŸæœ‰é€»è¾‘ï¼‰

        Args:
            constellation_visible_task_sets: æ˜Ÿåº§å¯è§ä»»åŠ¡é›†

        Returns:
            (å¤„ç†ä»»åŠ¡æ•°, é‡‡é›†ä½ç½®ç‚¹æ•°)
        """
        total_tasks_processed = 0
        total_positions_collected = 0

        # éå†æ¯ä¸ªå«æ˜Ÿçš„å¯è§ä»»åŠ¡
        for satellite_id, satellite_tasks in constellation_visible_task_sets.items():
            logger.info(f"ğŸ›°ï¸ å¤„ç†å«æ˜Ÿ {satellite_id} çš„å¯è§ä»»åŠ¡...")

            # å¤„ç†è¯¥å«æ˜Ÿçš„æ‰€æœ‰å¯¼å¼¹ä»»åŠ¡
            missile_tasks = satellite_tasks.get("missile_tasks", {})

            for missile_id, missile_task_data in missile_tasks.items():
                # å¤„ç†å¯è§ä»»åŠ¡
                visible_tasks = missile_task_data.get("visible_tasks", [])

                for task in visible_tasks:
                    # ä¸ºæ¯ä¸ªå¯è§ä»»åŠ¡åŒæ­¥å«æ˜Ÿä½ç½®
                    position_data = self._synchronize_position_for_task(
                        satellite_id, task
                    )

                    if position_data:
                        task["satellite_position_sync"] = position_data
                        total_positions_collected += len(position_data.get("position_samples", []))
                        total_tasks_processed += 1

                        logger.debug(f"âœ… ä»»åŠ¡ {task.get('task_id')} ä½ç½®åŒæ­¥å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ ä»»åŠ¡ {task.get('task_id')} ä½ç½®åŒæ­¥å¤±è´¥")

        return total_tasks_processed, total_positions_collected

    def _synchronize_position_for_task_thread_safe(self, satellite_id: str, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        çº¿ç¨‹å®‰å…¨çš„ä»»åŠ¡ä½ç½®åŒæ­¥æ–¹æ³•

        Args:
            satellite_id: å«æ˜ŸID
            task: ä»»åŠ¡ä¿¡æ¯

        Returns:
            ä½ç½®åŒæ­¥æ•°æ®
        """
        try:
            # ä½¿ç”¨é”ç¡®ä¿STK COMæ¥å£çš„çº¿ç¨‹å®‰å…¨
            with self._stk_lock:
                return self._synchronize_position_for_task(satellite_id, task)
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹å®‰å…¨ä½ç½®åŒæ­¥å¤±è´¥ {task.get('task_id')}: {e}")
            return None

    def _synchronize_position_for_task(self, satellite_id: str, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ä¸ºå•ä¸ªä»»åŠ¡åŒæ­¥å«æ˜Ÿä½ç½®
        
        Args:
            satellite_id: å«æ˜ŸID
            task: ä»»åŠ¡ä¿¡æ¯
            
        Returns:
            ä½ç½®åŒæ­¥æ•°æ®
        """
        try:
            # è·å–ä»»åŠ¡æ—¶é—´èŒƒå›´
            start_time_str = task.get("start_time_iso") or task.get("start_time")
            end_time_str = task.get("end_time_iso") or task.get("end_time")
            
            if not start_time_str or not end_time_str:
                logger.warning(f"âš ï¸ ä»»åŠ¡æ—¶é—´ä¿¡æ¯ä¸å®Œæ•´: {task.get('task_id')}")
                return None
            
            # è§£ææ—¶é—´
            start_time = self._parse_time_string(start_time_str)
            end_time = self._parse_time_string(end_time_str)
            
            if not start_time or not end_time:
                logger.warning(f"âš ï¸ ä»»åŠ¡æ—¶é—´è§£æå¤±è´¥: {task.get('task_id')}")
                return None
            
            # è®¡ç®—é‡‡æ ·æ—¶é—´ç‚¹
            sample_times = self._calculate_sample_times(start_time, end_time)
            
            # é‡‡é›†ä½ç½®æ•°æ®
            position_samples = []
            
            for sample_time in sample_times:
                # è®¡ç®—ç›¸å¯¹äºåœºæ™¯å¼€å§‹æ—¶é—´çš„åç§»
                time_offset = self._calculate_time_offset(sample_time)
                
                # è·å–å«æ˜Ÿåœ¨è¯¥æ—¶é—´ç‚¹çš„ä½ç½®ï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰
                position_data = self.stk_manager.get_satellite_position(satellite_id, time_offset, self.stk_com_timeout)
                
                if position_data:
                    # å¢å¼ºä½ç½®æ•°æ®
                    enhanced_position = {
                        "sample_time": sample_time.isoformat(),
                        "time_offset_seconds": time_offset,
                        "position": position_data,
                        "task_relative_time": (sample_time - start_time).total_seconds()
                    }
                    position_samples.append(enhanced_position)
                else:
                    logger.debug(f"âš ï¸ æ— æ³•è·å– {satellite_id} åœ¨ {sample_time} çš„ä½ç½®")
            
            if not position_samples:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task.get('task_id')} æœªè·å–åˆ°ä»»ä½•ä½ç½®æ•°æ®")
                return None
            
            # è®¡ç®—ä½ç½®ç»Ÿè®¡ä¿¡æ¯
            position_stats = self._calculate_position_statistics(position_samples)
            
            return {
                "task_id": task.get("task_id"),
                "satellite_id": satellite_id,
                "task_time_range": {
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": (end_time - start_time).total_seconds()
                },
                "position_samples": position_samples,
                "position_statistics": position_stats,
                "sample_count": len(position_samples),
                "sample_interval_seconds": self.position_sample_interval
            }
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ä½ç½®åŒæ­¥å¤±è´¥ {task.get('task_id')}: {e}")
            return None
    
    def _parse_time_string(self, time_str: str) -> Optional[datetime]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            # å°è¯•ISOæ ¼å¼
            if 'T' in time_str:
                return datetime.fromisoformat(time_str.replace('T', ' '))
            else:
                return datetime.fromisoformat(time_str)
        except:
            try:
                # å°è¯•æ ‡å‡†æ ¼å¼
                return datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
            except:
                logger.warning(f"âš ï¸ æ— æ³•è§£ææ—¶é—´å­—ç¬¦ä¸²: {time_str}")
                return None
    
    def _calculate_sample_times(self, start_time: datetime, end_time: datetime) -> List[datetime]:
        """
        è®¡ç®—é‡‡æ ·æ—¶é—´ç‚¹

        ğŸš€ ä¼˜åŒ–ç­–ç•¥ï¼šå¯¹äºå¯è§å…ƒä»»åŠ¡ï¼Œåªé‡‡é›†å¼€å§‹å’Œç»“æŸæ—¶åˆ»çš„ä½ç½®ä¿¡æ¯
        è¿™æ ·å¯ä»¥å¤§å¤§åŠ é€Ÿæ•°æ®é‡‡é›†é€Ÿåº¦ï¼Œä»699ä¸ªè¯·æ±‚å‡å°‘åˆ°128ä¸ªè¯·æ±‚ï¼ˆ64ä¸ªä»»åŠ¡ Ã— 2ä¸ªæ—¶é—´ç‚¹ï¼‰
        """
        sample_times = []

        # ä»»åŠ¡æŒç»­æ—¶é—´
        duration = (end_time - start_time).total_seconds()

        # ğŸš€ ä¼˜åŒ–ï¼šå¯¹äºå¯è§å…ƒä»»åŠ¡ï¼Œåªé‡‡é›†å¼€å§‹å’Œç»“æŸæ—¶åˆ»çš„ä½ç½®
        # è¿™æ˜¯æœ€é«˜æ•ˆçš„ç­–ç•¥ï¼Œæ»¡è¶³ä½ç½®ä¿¡æ¯éœ€æ±‚çš„åŒæ—¶æœ€å¤§åŒ–é‡‡é›†é€Ÿåº¦
        sample_times = [start_time]
        if start_time != end_time:
            sample_times.append(end_time)

        logger.debug(f"ğŸ“ ä»»åŠ¡é‡‡æ ·ç­–ç•¥: æŒç»­æ—¶é—´{duration:.1f}s, é‡‡æ ·ç‚¹æ•°: {len(sample_times)}")

        return sample_times
    
    def _calculate_time_offset(self, target_time: datetime) -> float:
        """è®¡ç®—ç›¸å¯¹äºåœºæ™¯å¼€å§‹æ—¶é—´çš„åç§»é‡ï¼ˆç§’ï¼‰"""
        try:
            scenario_start = self.time_manager.start_time
            offset = (target_time - scenario_start).total_seconds()
            return offset
        except Exception as e:
            logger.warning(f"âš ï¸ æ—¶é—´åç§»è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_position_statistics(self, position_samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ä½ç½®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not position_samples:
                return {}
            
            # æå–åæ ‡æ•°æ®
            coordinates = []
            for sample in position_samples:
                pos = sample.get("position", {})
                if "x" in pos and "y" in pos and "z" in pos:
                    coordinates.append((pos["x"], pos["y"], pos["z"]))
                elif "lat" in pos and "lon" in pos and "alt" in pos:
                    coordinates.append((pos["lat"], pos["lon"], pos["alt"]))
            
            if not coordinates:
                return {"error": "no_valid_coordinates"}
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if len(coordinates[0]) == 3:  # ç¬›å¡å°”åæ ‡æˆ–LLAåæ ‡
                x_coords, y_coords, z_coords = zip(*coordinates)
                
                stats = {
                    "coordinate_type": "cartesian" if abs(max(x_coords)) > 1000 else "lla",
                    "x_range": {"min": min(x_coords), "max": max(x_coords)},
                    "y_range": {"min": min(y_coords), "max": max(y_coords)},
                    "z_range": {"min": min(z_coords), "max": max(z_coords)},
                    "sample_count": len(coordinates)
                }
                
                # è®¡ç®—ç§»åŠ¨è·ç¦»ï¼ˆå¦‚æœæ˜¯ç¬›å¡å°”åæ ‡ï¼‰
                if stats["coordinate_type"] == "cartesian":
                    total_distance = 0
                    for i in range(1, len(coordinates)):
                        dx = coordinates[i][0] - coordinates[i-1][0]
                        dy = coordinates[i][1] - coordinates[i-1][1]
                        dz = coordinates[i][2] - coordinates[i-1][2]
                        distance = (dx**2 + dy**2 + dz**2)**0.5
                        total_distance += distance
                    
                    stats["total_movement_km"] = total_distance / 1000  # è½¬æ¢ä¸ºå…¬é‡Œ
                
                return stats
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä½ç½®ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            return {"error": str(e)}
        
        return {}
    
    def export_position_sync_data(self, enhanced_visible_tasks: Dict[str, Any], output_file: str):
        """å¯¼å‡ºä½ç½®åŒæ­¥æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_visible_tasks, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ“ ä½ç½®åŒæ­¥æ•°æ®å·²å¯¼å‡º: {output_file}")
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºä½ç½®åŒæ­¥æ•°æ®å¤±è´¥: {e}")

    def _generate_position_requests_for_task(self, satellite_id: str, task: Dict[str, Any]) -> List[PositionRequest]:
        """
        ä¸ºå•ä¸ªä»»åŠ¡ç”Ÿæˆä½ç½®è¯·æ±‚åˆ—è¡¨

        Args:
            satellite_id: å«æ˜ŸID
            task: ä»»åŠ¡ä¿¡æ¯

        Returns:
            ä½ç½®è¯·æ±‚åˆ—è¡¨
        """
        try:
            # è§£æä»»åŠ¡æ—¶é—´èŒƒå›´ - ä¼˜å…ˆä½¿ç”¨ISOæ ¼å¼
            start_time_str = task.get("start_time_iso") or task.get("start_time")
            end_time_str = task.get("end_time_iso") or task.get("end_time")
            task_id = task.get("task_id")

            if not start_time_str or not end_time_str:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} æ—¶é—´èŒƒå›´æ— æ•ˆ")
                return []

            # è½¬æ¢æ—¶é—´æ ¼å¼ - å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
            try:
                # é¦–å…ˆå°è¯•ISOæ ¼å¼
                start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                end_time = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
            except ValueError:
                # å¦‚æœISOæ ¼å¼å¤±è´¥ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
                start_time = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
                end_time = datetime.strptime(end_time_str, "%Y-%m-%d %H:%M:%S")

            # è®¡ç®—é‡‡æ ·æ—¶é—´ç‚¹
            sample_times = self._calculate_sample_times(start_time, end_time)

            # ç”Ÿæˆä½ç½®è¯·æ±‚
            requests = []
            for sample_time in sample_times:
                time_offset = self._calculate_time_offset(sample_time)

                request = PositionRequest(
                    satellite_id=satellite_id,
                    time_offset=time_offset,
                    sample_time=sample_time,
                    task_id=task_id,
                    priority=1  # é«˜ä¼˜å…ˆçº§
                )
                requests.append(request)

            return requests

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä»»åŠ¡ {task.get('task_id')} ä½ç½®è¯·æ±‚å¤±è´¥: {e}")
            return []

    def _build_position_sync_data_from_results(self, satellite_id: str, task: Dict[str, Any],
                                             position_results: List[PositionResult]) -> Optional[Dict[str, Any]]:
        """
        ä»ä½ç½®ç»“æœæ„å»ºä½ç½®åŒæ­¥æ•°æ®

        Args:
            satellite_id: å«æ˜ŸID
            task: ä»»åŠ¡ä¿¡æ¯
            position_results: ä½ç½®ç»“æœåˆ—è¡¨

        Returns:
            ä½ç½®åŒæ­¥æ•°æ®
        """
        try:
            # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
            successful_results = [r for r in position_results if r.success and r.position_data]

            if not successful_results:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task.get('task_id')} æ²¡æœ‰æˆåŠŸçš„ä½ç½®æ•°æ®")
                return None

            # æ„å»ºä½ç½®æ ·æœ¬
            position_samples = []

            # è§£æä»»åŠ¡å¼€å§‹æ—¶é—´ç”¨äºè®¡ç®—ç›¸å¯¹æ—¶é—´
            task_start_time_str = task.get("start_time_iso") or task.get("start_time")
            try:
                # é¦–å…ˆå°è¯•ISOæ ¼å¼
                task_start_time = datetime.fromisoformat(task_start_time_str.replace('Z', '+00:00'))
            except ValueError:
                # å¦‚æœISOæ ¼å¼å¤±è´¥ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
                task_start_time = datetime.strptime(task_start_time_str, "%Y-%m-%d %H:%M:%S")

            for result in successful_results:
                enhanced_position = {
                    "sample_time": result.request.sample_time.isoformat(),
                    "time_offset_seconds": result.request.time_offset,
                    "position": result.position_data,
                    "task_relative_time": (result.request.sample_time - task_start_time).total_seconds(),
                    "processing_time": result.processing_time
                }
                position_samples.append(enhanced_position)

            # è®¡ç®—ä½ç½®ç»Ÿè®¡ä¿¡æ¯
            position_stats = self._calculate_position_statistics(position_samples)

            # è§£æä»»åŠ¡æ—¶é—´èŒƒå›´ - ä¼˜å…ˆä½¿ç”¨ISOæ ¼å¼
            start_time_str = task.get("start_time_iso") or task.get("start_time")
            end_time_str = task.get("end_time_iso") or task.get("end_time")

            # è½¬æ¢æ—¶é—´æ ¼å¼ - å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
            try:
                # é¦–å…ˆå°è¯•ISOæ ¼å¼
                start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                end_time = datetime.fromisoformat(end_time_str.replace('Z', '+00:00'))
            except ValueError:
                # å¦‚æœISOæ ¼å¼å¤±è´¥ï¼Œå°è¯•æ ‡å‡†æ ¼å¼
                start_time = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
                end_time = datetime.strptime(end_time_str, "%Y-%m-%d %H:%M:%S")

            return {
                "task_id": task.get("task_id"),
                "satellite_id": satellite_id,
                "task_time_range": {
                    "start_time": start_time.isoformat(),
                    "end_time": end_time.isoformat(),
                    "duration_seconds": (end_time - start_time).total_seconds()
                },
                "position_samples": position_samples,
                "position_statistics": position_stats,
                "sample_count": len(position_samples),
                "sample_interval_seconds": self.position_sample_interval,
                "parallel_processing": True,
                "success_rate": len(successful_results) / len(position_results) * 100 if position_results else 0
            }

        except Exception as e:
            logger.error(f"âŒ æ„å»ºä»»åŠ¡ {task.get('task_id')} ä½ç½®åŒæ­¥æ•°æ®å¤±è´¥: {e}")
            return None

    def get_parallel_performance_stats(self) -> Dict[str, Any]:
        """è·å–å¹¶è¡Œå¤„ç†æ€§èƒ½ç»Ÿè®¡"""
        if hasattr(self, 'parallel_position_manager'):
            return self.parallel_position_manager.get_stats()
        return {}

    def clear_parallel_cache(self):
        """æ¸…ç©ºå¹¶è¡Œå¤„ç†ç¼“å­˜"""
        if hasattr(self, 'parallel_position_manager'):
            self.parallel_position_manager.clear_cache()
            logger.info("ğŸ§¹ å¹¶è¡Œä½ç½®ç¼“å­˜å·²æ¸…ç©º")
